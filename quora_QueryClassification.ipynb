{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of quora_load_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itspritish/nlp/blob/master/quora_QueryClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8lL-GFhSJBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "     !pip install -U -q PyDrive ## you will have install for every colab session\n",
        "\n",
        "     from pydrive.auth import GoogleAuth\n",
        "     from pydrive.drive import GoogleDrive\n",
        "     from google.colab import auth\n",
        "     from oauth2client.client import GoogleCredentials\n",
        "\n",
        "     # 1. Authenticate and create the PyDrive client.\n",
        "     auth.authenticate_user()\n",
        "     gauth = GoogleAuth()\n",
        "     gauth.credentials = GoogleCredentials.get_application_default()\n",
        "     drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLkCXUCOSM1S",
        "colab_type": "code",
        "outputId": "fbc31d37-a8bf-4071-bb10-cc33304ec4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vbnu4pkSYTa",
        "colab_type": "code",
        "outputId": "0ada02a4-7d4f-4188-db56-822aba85f270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1CIq_rNSPTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "os.chdir(\"drive/My Drive/TCS/Quora_q\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUA4Wh19SBSd",
        "colab_type": "code",
        "outputId": "fe958f9d-f20b-4479-bc36-7428607a481c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model,Sequential\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from sklearn.metrics import confusion_matrix as cf\n",
        "\n",
        "import sys\n",
        "stdout = sys.stdout\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOzYKkjmSBSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BASE_DIR = 'C:\\\\videoken\\\\walmart\\\\assignment5/week7_assignment_data/'\n",
        "TRAIN_DATA_FILE = 'train.csv'\n",
        "TEST_DATA_FILE = 'test.csv'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8qxZE4MSBSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCtvB3AjSBSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_pad_sequences(data_col):\n",
        "    sequences = tokenizer.texts_to_sequences(data_col)\n",
        "    d_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(d_index))\n",
        "    data_seq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    return data_seq\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7BRd5gVKHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT97z55ec6FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpdeaWzkO58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('questions.csv',usecols=['question1','question2','is_duplicate'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaJ42G-MSBS2",
        "colab_type": "text"
      },
      "source": [
        "## Read data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx966qLaSBS4",
        "colab_type": "code",
        "outputId": "39dfffb6-0bc8-41a4-a3b4-5ad74df6fdc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test_withlabels.csv')\n",
        "texts = train['question1'].tolist()\n",
        "texts.extend(train['question2'].tolist())\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "\n",
        "print(train['question1'].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 15579 unique tokens.\n",
            "Shape of data tensor: (20000, 100)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyN-yxcZitok",
        "colab_type": "code",
        "outputId": "bc247acf-660b-4958-c45a-d9e6cc742e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_q1 = train['question1'].tolist()\n",
        "train_q2 = train['question2'].tolist()\n",
        "\n",
        "x_train_1 = prepare_pad_sequences(train_q1)\n",
        "x_train_2 = prepare_pad_sequences(train_q2)\n",
        "\n",
        "train_label = train['is_duplicate']\n",
        "\n",
        "test_q1 = test['question1'].tolist()\n",
        "test_q2 = test['question2'].tolist()\n",
        "\n",
        "x_test_1 = prepare_pad_sequences(test_q1)\n",
        "x_test_2 = prepare_pad_sequences(test_q2)\n",
        "\n",
        "test_label = test['is_duplicate']\n",
        "\n",
        "texts = x_train_1\n",
        "#pd.concat(texts,x_train_2)\n",
        "print(x_train_1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15579 unique tokens.\n",
            "Found 15579 unique tokens.\n",
            "Found 15579 unique tokens.\n",
            "Found 15579 unique tokens.\n",
            "[[   0    0    0 ...  449    8   36]\n",
            " [   0    0    0 ...    4 6070 3988]\n",
            " [   0    0    0 ...  157    7 3064]\n",
            " ...\n",
            " [   0    0    0 ...  847 1249  653]\n",
            " [   0    0    0 ...   40   14 8705]\n",
            " [   0    0    0 ...  392 1562  919]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrdRQAYtV-NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\"\"\"config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "set_session(tf.Session(config=config))\n",
        "\"\"\"\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, concatenate, dot\n",
        "max_features=3000\n",
        "embedding_size = 100\n",
        "inp1 = Input(shape=(100,))\n",
        "inp2 = Input(shape=(100,))\n",
        "\n",
        "x1 = Embedding(max_features, embedding_size)(inp1)\n",
        "x2 = Embedding(max_features, embedding_size)(inp2)\n",
        "\n",
        "x3 = LSTM(32, return_sequences = True)(x1)\n",
        "x4 = LSTM(32, return_sequences = True)(x2)\n",
        "\n",
        "x5 = GlobalMaxPool1D()(x3)\n",
        "x6 = GlobalMaxPool1D()(x4)\n",
        "\n",
        "x7 = dot([x5, x6], axes=1)\n",
        "\n",
        "x8 = Dense(40, activation='relu')(x7)\n",
        "x9 = Dropout(0.05)(x8)\n",
        "x10 = Dense(10, activation='relu')(x9)\n",
        "output = Dense(1, activation='sigmoid')(x10)\n",
        "\n",
        "model = Model(inputs=[inp1, inp2], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "batch_size = 8\n",
        "epochs = 5\n",
        "\n",
        "#model.fit([x_train_1, x_train_2], train_label, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVZpcBIYAmE-",
        "colab_type": "code",
        "outputId": "0d8ec9e4-ccca-4007-a215-91218c81db1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "\n",
        "model=keras.models.load_model('quora.h5')\n",
        "model.fit([x_train_1, x_train_2], train_label, batch_size=8, epochs=5, validation_split=0.2)\n",
        "\n",
        "#x_mode = model.predict([x_train_1, x_train_2])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/5\n",
            "8000/8000 [==============================] - 546s 68ms/step - loss: 0.6628 - acc: 0.6266 - val_loss: 0.6570 - val_acc: 0.6340\n",
            "Epoch 2/5\n",
            "8000/8000 [==============================] - 542s 68ms/step - loss: 0.6603 - acc: 0.6276 - val_loss: 0.6572 - val_acc: 0.6340\n",
            "Epoch 3/5\n",
            "8000/8000 [==============================] - 547s 68ms/step - loss: 0.6606 - acc: 0.6276 - val_loss: 0.6570 - val_acc: 0.6340\n",
            "Epoch 4/5\n",
            "8000/8000 [==============================] - 557s 70ms/step - loss: 0.6605 - acc: 0.6276 - val_loss: 0.6569 - val_acc: 0.6340\n",
            "Epoch 5/5\n",
            "8000/8000 [==============================] - 548s 69ms/step - loss: 0.6605 - acc: 0.6276 - val_loss: 0.6569 - val_acc: 0.6340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9b212a7f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5aaxJeEHZgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_mode=(x_mode>0.5)+[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FbVWPS6Hrxp",
        "colab_type": "code",
        "outputId": "0e8c1449-204b-413a-a4c0-0abc3437e0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(train_label,x_mode)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9EvaxMxizOR",
        "colab_type": "text"
      },
      "source": [
        "# **SEQUENTIAL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkwNJetGivrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "inp1 = Input(shape=(100,))\n",
        "inp2 = Input(shape=(100,))\n",
        "\n",
        "x1 = Embedding(max_features, embedding_size)(inp1)\n",
        "x2 = Embedding(max_features, embedding_size)(inp2)\n",
        "\n",
        "\n",
        "input_dim = x_train_1.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(15,input_dim=input_dim,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OvJ5cMlzTVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E81OmR2Z0oFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_1.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xTYqw7V039Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1 = model.fit(x_test_1,train_label,epochs=35,verbose=False,validation_data=(x_test_1,test_label),batch_size=130)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p4os9lc40bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss,accuracy = model.evaluate(x_train_1,train_label,verbose=False)\n",
        "print(\"Training accuracy:\",accuracy)\n",
        "loss,accuracy = model.evaluate(x_test_1,test_label,verbose=False)\n",
        "print(\"Test accuracy:\",accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}